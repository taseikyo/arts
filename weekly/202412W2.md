> @Author  : Lewis Tian (taseikyo@gmail.com)
>
> @Link    : github.com/taseikyo
>
> @Range   : 2024-12-08 - 2024-12-14

# Weekly #92

[readme](../README.md) | [previous](202412W1.md) | [next](202412W3.md)

![](../images/2024/12/jeferson-gomes-Ymq0SnUNN7A-unsplash.jpg "Weekly #92")

\**Photo by [JEFERSON GOMES](https://unsplash.com/@daluz) on [Unsplash](https://unsplash.com/photos/woman-in-blue-denim-vest-wearing-black-sunglasses-Ymq0SnUNN7A)*

## Table of Contents

- [algorithm](#algorithm-)
- [review](#review-)
	- e820简介
	- 烂大街的缓存穿透、缓存击穿和缓存雪崩，你真的懂了？
- [tip](#tip-)
- [share](#share-)

## algorithm [🔝](#weekly-92)

## review [🔝](#weekly-92)

### 1. [e820简介](https://blog.csdn.net/gxfan/article/details/2962236)

e820 是和 BIOS 的一个中断相关的，具体说是 int 0x15。之所以叫 e820 是因为在用这个中断时 ax 必须是 0xe820。这个中断的作用是得到系统的内存布局。因为系统内存会有很多段，每段的类型属性也不一样，所以这个查询是 “迭代式” 的，每次求得一个段。

我们看内核源代码。主要涉及两个文件：`arch/x86/boot/memory.c` 和 `arch/x86/kernel/e820_32.c`。我们 已经很幸运了，这部分代码已经用 C 重写过了。你可能会奇怪，启动调用 e820 时我们还在实模式，怎么能用 C 呢？答案是，这里用的是 16 位的 C。gcc 早已 经支持. code16 gcc 模式了。

看 detect_memory_e820() 函数，里面就是 e820 的本质。它把 int 0x15 放到一个 do-while 循环里，每次得到的一个内存段放到 `struct e820entry` 里，而 `struct e820entry` 的结构正是 e820 返回结果的结构！而像其它启动时获得的结果一样，最终都会被放到 boot_params 里，e820 被放到了 `boot_params.e820_map`。

如果你对 `struct e820entry` 还有疑问，你可以看一下 `arch/x86/kernel/e820_32.c::print_memory_map()`，看看里面是怎么使用它的。

当然了，在 `arch/x86/boot/memory.c` 里，你还会看到另外两个利用 int 0x15 查询内存的函数，不过用途不一样了。

附：

boot_params 结构体定义，其中 E820MAX 定义为 128:

```c
struct e820entry {
	__u64 addr;
	/* start of memory segment */
	__u64 size;
	/* size of memory segment */
	__u32 type;
	/* type of memory segment */
}
__attribute__((packed));
struct boot_params {
	struct screen_info screen_info;
	/* 0x000 */
	struct apm_bios_info apm_bios_info;
	/* 0x040 */
	__u8  _pad2[12];
	/* 0x054 */
	struct ist_info ist_info;
	/* 0x060 */
	__u8  _pad3[16];
	/* 0x070 */
	__u8  hd0_info[16];
	/* obsolete! */
	/* 0x080 */
	__u8  hd1_info[16];
	/* obsolete! */
	/* 0x090 */
	struct sys_desc_table sys_desc_table;
	/* 0x0a0 */
	__u8  _pad4[144];
	/* 0x0b0 */
	struct edid_info edid_info;
	/* 0x140 */
	struct efi_info efi_info;
	/* 0x1c0 */
	__u32 alt_mem_k;
	/* 0x1e0 */
	__u32 scratch;
	/* Scratch field! */
	/* 0x1e4 */
	__u8  e820_entries;
	/* 0x1e8 */
	__u8  eddbuf_entries;
	/* 0x1e9 */
	__u8  edd_mbr_sig_buf_entries;
	/* 0x1ea */
	__u8  _pad6[6];
	/* 0x1eb */
	struct setup_header hdr;
	/* setup header */
	/* 0x1f1 */
	__u8  _pad7[0x290-0x1f1-sizeof(struct setup_header)];
	__u32 edd_mbr_sig_buffer[EDD_MBR_SIG_MAX];
	/* 0x290 */
	struct e820entry e820_map[E820MAX];
	/* 0x2d0 */
	__u8  _pad8[48];
	/* 0xcd0 */
	struct edd_info eddbuf[EDDMAXNR];
	/* 0xd00 */
	__u8  _pad9[276];
	/* 0xeec */
}
__attribute__((packed));
```

通过 bios 获取系统内存布局代码如下：

```C
static int detect_memory_e820(void) {
	int count = 0;
	u32 next = 0;
	u32 size, id;
	u8 err;
	struct e820entry *desc = boot_params.e820_map;
	do {
		size = sizeof(struct e820entry);
		/* Important: %edx is clobbered by some BIOSes,
		so it must be either used for the error output
		or explicitly marked clobbered. */
		asm("int $0x15; setc %0"
			: "=d" (err), "+b" (next), "=a" (id), "+c" (size), "=m" (*desc)
			: "D" (desc), "d" (SMAP), "a" (0xe820));
		/* BIOSes which terminate the chain with CF = 1 as opposed
		to %ebx = 0 don't always report the SMAP signature on
		the final, failing, probe. */
		if (err)
				 break;
		/* Some BIOSes stop returning SMAP in the middle of
		the search loop.  We don't know exactly how the BIOS
		screwed up the map at that point, we might have a
		partial map, the full map, or complete garbage, so
		just return failure. */
		if (id != SMAP) {
			count = 0;
			break;
		}
		count++;
		desc++;
	}
	while (next && count < E820MAX);
	return boot_params.e820_entries = count;
}
```

这个函数执行完毕后，boot_params.e820_map 就含有了系统内存布局图。

函数关键部分解释如下：

07 获取启动参数 boot_params 里的 e820_map 数组首地址。

15-18 通过中断 0x15 调用 bios 例程获得一个内存段的信息，这条语句是按照 AT&T 的汇编语法格式写的，具体语法可以查看相关资料。

### 2. [烂大街的缓存穿透、缓存击穿和缓存雪崩，你真的懂了？](https://www.cnblogs.com/12lisu/p/15732213.html)

#### 1、缓存穿透问题

大部分情况下，加缓存的目的是：为了减轻数据库的压力，提升系统的性能。

1.1 我们是如何用缓存的？

一般情况下，如果有用户请求过来，先查缓存，如果缓存中存在数据，则直接返回。如果缓存中不存在，则再查数据库，如果数据库中存在，则将数据放入缓存，然后返回。如果数据库中也不存在，则直接返回失败。

![](../images/2024/12/edcb8d41-a98b-45ca-8121-1a77f33b697c.jpg)


1.2 什么是缓存穿透？

但如果出现以下这两种特殊情况，比如：

- 用户请求的 id 在缓存中不存在。
- 恶意用户伪造不存在的 id 发起请求。

这样的用户请求导致的结果是：每次从缓存中都查不到数据，而需要查询数据库，同时数据库中也没有查到该数据，也没法放入缓存。也就是说，每次这个用户请求过来的时候，都要查询一次数据库。

![](../images/2024/12/264b84d3-f032-4c5a-af73-0a03fc2c5753.jpg)

1.3 校验参数

我们可以对用户 id 做检验。

比如你的合法 id 是 15xxxxxx，以 15 开头的。如果用户传入了 16 开头的 id，比如：16232323，则参数校验失败，直接把相关请求拦截掉。这样可以过滤掉一部分恶意伪造的用户 id。

1.4 布隆过滤器

如果数据比较少，我们可以把数据库中的数据，全部放到内存的一个 map 中。这样能够非常快速的识别，数据在缓存中是否存在。如果存在，则让其访问缓存。如果不存在，则直接拒绝该请求。

但如果数据量太多了，有数千万或者上亿的数据，全都放到内存中，很显然会占用太多的内存空间。

那么，有没有办法减少内存空间呢？

答：这就需要使用布隆过滤器了。

布隆过滤器底层使用 bit 数组存储数据，该数组中的元素默认值是 0。布隆过滤器第一次初始化的时候，会把数据库中所有已存在的 key，经过一些列的 hash 算法（比如：三次 hash 算法）计算，每个 key 都会计算出多个位置，然后把这些位置上的元素值设置成 1。

![](../images/2024/12/c1865319-8f21-4e57-b215-e1fc8b22326c.jpg)

之后，有用户 key 请求过来的时候，再用相同的 hash 算法计算位置。

- 如果多个位置中的元素值都是 1，则说明该 key 在数据库中已存在。这时允许继续往后面操作。
- 如果有 1 个以上的位置上的元素值是 0，则说明该 key 在数据库中不存在。这时可以拒绝该请求，而直接返回。

使用布隆过滤器确实可以解决缓存穿透问题，但同时也带来了两个问题：

- 存在误判的情况。
- 存在数据更新问题。

先看看为什么会存在误判呢？

上面我已经说过，初始化数据时，针对每个 key 都是通过多次 hash 算法，计算出一些位置，然后把这些位置上的元素值设置成 1。

但我们都知道 hash 算法是会出现 hash 冲突的，也就是说不同的 key，可能会计算出相同的位置。

![](../images/2024/12/aed7a9de-f98f-47ae-8d0a-cb3d56184d27.jpg)

上图中的下标为 2 的位置就出现了 hash 冲突，key1 和 key2 计算出了一个相同的位置。

如果有几千万或者上亿的数据，布隆过滤器中的 hash 冲突会非常明显。

如果某个用户 key，经过多次 hash 计算出的位置，其元素值，恰好都被其他的 key 初始化成了 1。此时，就出现了误判，原本这个 key 在数据库中是不存在的，但布隆过滤器确认为存在。

> 如果布隆过滤器判断出某个 key 存在，可能出现误判。如果判断某个 key 不存在，则它在数据库中一定不存在。

通常情况下，布隆过滤器的误判率还是比较少的。即使有少部分误判的请求，直接访问了数据库，但如果访问量并不大，对数据库影响也不大。

此外，如果想减少误判率，可以适当增加 hash 函数，图中用的 3 次 hash，可以增加到 5 次。

其实，布隆过滤器最致命的问题是：如果数据库中的数据更新了，需要同步更新布隆过滤器。但它跟数据库是两个数据源，就可能存在数据不一致的情况。

比如：数据库中新增了一个用户，该用户数据需要实时同步到布隆过滤。但由于网络异常，同步失败了。

![](../images/2024/12/9864e5f4-e97e-489b-943b-339874a70af0.jpg)

这时刚好该用户请求过来了，由于布隆过滤器没有该 key 的数据，所以直接拒绝了该请求。但这个是正常的用户，也被拦截了。

很显然，如果出现了这种正常用户被拦截了情况，有些业务是无法容忍的。所以，布隆过滤器要看实际业务场景再决定是否使用，它帮我们解决了缓存穿透问题，但同时了带来了新的问题。


1.5 缓存空值

上面使用布隆过滤器，虽说可以过滤掉很多不存在的用户 id 请求。但它除了增加系统的复杂度之外，会带来两个问题：

- 布隆过滤器存在误杀的情况，可能会把少部分正常用户的请求也过滤了。
- 如果用户信息有变化，需要实时同步到布隆过滤器，不然会有问题。

所以，通常情况下，我们很少用布隆过滤器解决缓存穿透问题。其实，还有另外一种更简单的方案，即：缓存空值。

当某个用户 id 在缓存中查不到，在数据库中也查不到时，也需要将该用户 id 缓存起来，只不过值是空的。这样后面的请求，再拿相同的用户 id 发起请求时，就能从缓存中获取空数据，直接返回了，而无需再去查一次数据库。

优化之后的流程图如下：

![](../images/2024/12/4213f660-327e-4aeb-9aad-a4d0ac4d89df.jpg)

关键点是不管从数据库有没有查到数据，都将结果放入缓存中，只是如果没有查到数据，缓存中的值是空的罢了。

#### 2、缓存击穿问题

2.1 什么是缓存击穿？

有时候，我们在访问热点数据时。比如：我们在某个商城购买某个热门商品。

为了保证访问速度，通常情况下，商城系统会把商品信息放到缓存中。但如果某个时刻，该商品到了过期时间失效了。

此时，如果有大量的用户请求同一个商品，但该商品在缓存中失效了，一下子这些用户请求都直接怼到数据库，可能会造成瞬间数据库压力过大，而直接挂掉。

流程图如下：

![](../images/2024/12/11a098e3-db86-47b9-830d-71740e8efc58.jpg)

2.2 加锁

数据库压力过大的根源是，因为同一时刻太多的请求访问了数据库。

如果我们能够限制，同一时刻只有一个请求才能访问某个 productId 的数据库商品信息，不就能解决问题了？

答：没错，我们可以用加锁的方式，实现上面的功能。

2.3 自动续期

出现缓存击穿问题是由于 key 过期了导致的。那么，我们换一种思路，在 key 快要过期之前，就自动给它续期，不就 OK 了？

答：没错，我们可以用 job 给指定 key 自动续期。

比如说，我们有个分类功能，设置的缓存过期时间是 30 分钟。但有个 job 每隔 20 分钟执行一次，自动更新缓存，重新设置过期时间为 30 分钟。

此外，在很多请求第三方平台接口时，我们往往需要先调用一个获取 token 的接口，然后用这个 token 作为参数，请求真正的业务接口。一般获取到的 token 是有有效期的，比如 24 小时之后失效。

如果我们每次请求对方的业务接口，都要先调用一次获取 token 接口，显然比较麻烦，而且性能不太好。这时候，我们可以把第一次获取到的 token 缓存起来，请求对方业务接口时从缓存中获取 token。

同时，有一个 job 每隔一段时间，比如每隔 12 个小时请求一次获取 token 接口，不停刷新 token，重新设置 token 的过期时间。

2.4 缓存不失效

此外，对于很多热门 key，其实是可以不用设置过期时间，让其永久有效的。

比如参与秒杀活动的热门商品，由于这类商品 id 并不多，在缓存中我们可以不设置过期时间。在秒杀活动开始前，我们先用一个程序提前从数据库中查询出商品的数据，然后同步到缓存中，提前做预热。等秒杀活动结束一段时间之后，我们再手动删除这些无用的缓存即可。

#### 3、缓存雪崩问题

3.1 什么是缓存雪崩？

缓存雪崩是缓存击穿的升级版，缓存击穿说的是某一个热门 key 失效了，而缓存雪崩说的是有多个热门 key 同时失效。看起来，如果发生缓存雪崩，问题更严重。

缓存雪崩目前有两种：

- 有大量的热门缓存，同时失效。会导致大量的请求，访问数据库。而数据库很有可能因为扛不住压力，而直接挂掉。
- 缓存服务器 down 机了，可能是机器硬件问题，或者机房网络问题。总之，造成了整个缓存的不可用。

归根结底都是有大量的请求，透过缓存，而直接访问数据库了。

![](../images/2024/12/7aa68b22-612e-4de8-9aa2-d2c2b02d6079.jpg)

3.2 过期时间加随机数

为了解决缓存雪崩问题，我们首先要尽量避免缓存同时失效的情况发生。

这就要求我们不要设置相同的过期时间。可以在设置的过期时间基础上，再加个 1~60 秒的随机数。

3.3 高可用

针对缓存服务器 down 机的情况，在前期做系统设计时，可以做一些高可用架构。

比如：如果使用了 redis，可以使用哨兵模式，或者集群模式，避免出现单节点故障导致整个 redis 服务不可用的情况。使用哨兵模式之后，当某个 master 服务下线时，自动将该 master 下的某个 slave 服务升级为 master 服务，替代已下线的 master 服务继续处理请求。

3.4 服务降级

如果做了高可用架构，redis 服务还是挂了，该怎么办呢？

这时候，就需要做服务降级了。我们需要配置一些默认的兜底数据。程序中有个全局开关，比如有 10 个请求在最近一分钟内，从 redis 中获取数据失败，则全局开关打开。后面的新请求，就直接从配置中心中获取默认的数据。

![](../images/2024/12/00adc41d-3ff4-47c8-b319-e407d2cb33ac.jpg)

当然，还需要有个 job，每隔一定时间去从 redis 中获取数据，如果在最近一分钟内可以获取到两次数据（这个参数可以自己定），则把全局开关关闭。后面来的请求，又可以正常从 redis 中获取数据了。

需要特别说一句，该方案并非所有的场景都适用，需要根据实际业务场景决定。

## tip [🔝](#weekly-92)

## share [🔝](#weekly-92)

[readme](../README.md) | [previous](202412W1.md) | [next](202412W3.md)
